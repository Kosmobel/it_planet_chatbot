# AI_Chat_ver1

## Описание

Это серверное приложение, использующее FastAPI для обработки сообщений и взаимодействия с моделью на основе LlamaCPP. Сервер получает сообщения от клиента, обрабатывает их с использованием загруженной модели и возвращает ответы. Также поддерживает сохранение данных в формате JSON.

## Требования

Для работы сервера необходимо установить Python и несколько зависимостей. Убедитесь, что у вас установлен Python версии 3.7 или выше.

### Зависимости Python

Для работы приложения требуется установить следующие библиотеки:

- `fastapi` - для создания веб-приложения
- `uvicorn` - для запуска FastAPI приложения
- `pydantic` - для валидации данных
- `llama_cpp` - для работы с моделью Llama
- `asyncio` - для асинхронных операций
- `json` - для работы с данными в формате JSON

Вы можете установить все зависимости, используя `pip`. Для этого создайте файл `requirements.txt` со следующим содержимым: fastapi uvicorn pydantic llama-cpp-python


Затем выполните команду:

```bash
pip install -r requirements.txt


Дальше нужно просто запустить клиент на C# и выбрать путь до .GGUF модели, нажать кнопку загрузки. После этого можно общаться с моделью в режиме реального времени.


